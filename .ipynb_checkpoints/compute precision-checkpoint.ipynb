{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "from scipy.ndimage import label\n",
    "from tifffile import imread, imsave\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def pixel_sharing_bipartite(lab1, lab2):\n",
    "    assert lab1.shape == lab2.shape\n",
    "    psg = np.zeros((lab1.max()+1, lab2.max()+1), dtype=np.int)\n",
    "    for i in range(lab1.size):\n",
    "        psg[lab1.flat[i], lab2.flat[i]] += 1\n",
    "    return psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matching_iou(psg, fraction=0.5):\n",
    "    iou = intersection_over_union(psg)\n",
    "    matching = iou > fraction\n",
    "    matching[:,0] = False\n",
    "    matching[0,:] = False\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matching_max(psg):\n",
    "    \"\"\"\n",
    "    matching based on mutual first preference\n",
    "    \"\"\"\n",
    "    rowmax = np.argmax(psg, axis=0)\n",
    "    colmax = np.argmax(psg, axis=1)\n",
    "    starting_index = np.arange(psg.shape[1])\n",
    "    equal_matches = colmax[rowmax[starting_index]]==starting_index\n",
    "    rm, cm = rowmax[equal_matches], colmax[rowmax[equal_matches]]\n",
    "    matching = np.zeros_like(psg)\n",
    "    matching[rm, cm] = 1\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(psg):\n",
    "    rsum = np.sum(psg, 0, keepdims=True)\n",
    "    csum = np.sum(psg, 1, keepdims=True)\n",
    "    return psg / (rsum + csum - psg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matching_overlap(psg, fractions=(0.5,0.5)):\n",
    "    \"\"\"\n",
    "    create a matching given pixel_sharing_bipartite of two label images based on mutually overlapping regions of sufficient size.\n",
    "    NOTE: a true matching is only gauranteed for fractions > 0.5. Otherwise some cells might have deg=2 or more.\n",
    "    NOTE: doesnt break when the fraction of pixels matching is a ratio only slightly great than 0.5? (but rounds to 0.5 with float64?)\n",
    "    \"\"\"\n",
    "    afrac, bfrac = fractions\n",
    "    set0_object_sizes = np.sum(psg, axis=1, keepdims=True)\n",
    "    m0  = np.where(set0_object_sizes==0,0,psg / set0_object_sizes)\n",
    "    set1_object_sizes = np.sum(psg, axis=0, keepdims=True)\n",
    "    m1 = np.where(set1_object_sizes==0,0,psg / set1_object_sizes)\n",
    "    m0 = m0 > afrac\n",
    "    m1 = m1 > bfrac\n",
    "    matching = m0 * m1\n",
    "    matching = matching.astype('bool')\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(lab_gt, lab, iou=0.5, partial_dataset=False):\n",
    "    '''\n",
    "    precision = TP / (TP + FP + FN) i.e. \"intersection over union\" for a graph matching\n",
    "    '''\n",
    "    \n",
    "    psg = pixel_sharing_bipartite(lab_gt, lab)\n",
    "    matching = matching_iou(psg, fraction=iou)\n",
    "    assert matching.sum(0).max() < 2\n",
    "    assert matching.sum(1).max() < 2\n",
    "    n_gt  = len(set(np.unique(lab_gt)) - {0})\n",
    "    n_hyp = len(set(np.unique(lab)) - {0})\n",
    "    n_matched = matching.sum()\n",
    "#     print(\"TP:\", n_matched, \"FP:\", n_hyp-n_matched, \"FN:\", n_gt-n_matched)\n",
    "    if partial_dataset:\n",
    "        return n_matched , (n_gt + n_hyp - n_matched)\n",
    "    else:\n",
    "        return n_matched / (n_gt + n_hyp - n_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## full scores\n",
    "def seg(lab_gt, lab, partial_dataset=False):\n",
    "    \"\"\"\n",
    "    calculate seg from pixel_sharing_bipartite\n",
    "    seg is the average conditional-iou across ground truth cells\n",
    "    conditional-iou gives zero if not in matching\n",
    "    ----\n",
    "    calculate conditional intersection over union (CIoU) from matching & pixel_sharing_bipartite\n",
    "    for a fraction > 0.5 matching. Any CIoU between matching pairs will be > 1/3. But there may be some\n",
    "    IoU as low as 1/2 that don't match, and thus have CIoU = 0.\n",
    "    \"\"\"\n",
    "    psg = pixel_sharing_bipartite(lab_gt, lab)\n",
    "    iou = intersection_over_union(psg)\n",
    "    matching = matching_overlap(psg, fractions=(0.5, 0))\n",
    "    matching[0,:] = False\n",
    "    matching[:,0] = False\n",
    "    n_gt = len(set(np.unique(lab_gt)) - {0})\n",
    "    n_matched = iou[matching].sum()\n",
    "    if partial_dataset:\n",
    "        return n_matched , n_gt\n",
    "    else:\n",
    "        return n_matched / n_gt, (2* (n_matched / n_gt)/(1+(n_matched / n_gt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEG score : 0.827714473899\n",
      "DICE score : 0.905352369934\n"
     ]
    }
   ],
   "source": [
    "gt_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/SEG/*.tif\")\n",
    "label_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/BIC-MetaSeg/run1/temp/*.tif\")\n",
    "\n",
    "seg_scores_per_time = []\n",
    "dice_scores_per_time = []\n",
    "for i in range(len(gt_segs_path)):\n",
    "    seg_score, dice_score = seg(imread(gt_segs_path[i]), imread(label_segs_path[i]).astype(np.uint16))\n",
    "#     print(seg_score, dice_score)\n",
    "    seg_scores_per_time.append(seg_score)\n",
    "    dice_scores_per_time.append(dice_score)\n",
    "\n",
    "print(\"SEG score :\", np.mean(np.array(seg_scores_per_time) ))\n",
    "print(\"DICE score :\", np.mean(np.array(dice_scores_per_time) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP score for threshold 0.5 : 0.923076923077\n",
      "AP score for threshold 0.55 : 0.923076923077\n",
      "AP score for threshold 0.6 : 0.851851851852\n",
      "AP score for threshold 0.65 : 0.851851851852\n",
      "AP score for threshold 0.7 : 0.785714285714\n",
      "AP score for threshold 0.75 : 0.666666666667\n",
      "AP score for threshold 0.8 : 0.515151515152\n",
      "AP score for threshold 0.85 : 0.25\n",
      "AP score for threshold 0.9 : 0.0416666666667\n",
      "Mean AP score is:  0.645450742673\n"
     ]
    }
   ],
   "source": [
    "average_precision_score_list = []\n",
    "thresholds = np.arange(0.5,0.95,0.05)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    gt_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/SEG/*.tif\")[14:15]\n",
    "    label_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/BIC-MetaSeg/run1/67annotations/*.tif\")[14:15]\n",
    "    sum_seg = 0\n",
    "    scores_per_time = []\n",
    "    for i in range(len(gt_segs_path)):\n",
    "        score = precision(imread(label_segs_path[i]).astype(np.uint16), imread(gt_segs_path[i]), thresh)\n",
    "        scores_per_time.append(score)\n",
    "        if(0<=score<=1):\n",
    "            sum_seg = sum_seg + score\n",
    "            \n",
    "    average_precision_score = sum_seg/len(gt_segs_path)\n",
    "    average_precision_score_list.append(average_precision_score)\n",
    "    \n",
    "    print(\"AP score for threshold \"+str(thresh)+\" :\", average_precision_score )\n",
    "    \n",
    "print(\"Mean AP score is: \", np.mean(np.array(average_precision_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AP score for threshold 0.5 : 0.923076923077\n",
    "AP score for threshold 0.55 : 0.923076923077\n",
    "AP score for threshold 0.6 : 0.923076923077\n",
    "AP score for threshold 0.65 : 0.851851851852\n",
    "AP score for threshold 0.7 : 0.724137931034\n",
    "AP score for threshold 0.75 : 0.724137931034\n",
    "AP score for threshold 0.8 : 0.5625\n",
    "AP score for threshold 0.85 : 0.190476190476\n",
    "AP score for threshold 0.9 : 0.063829787234\n",
    "Mean AP score is:  0.654018273429\n",
    "    \n",
    "AP score for threshold 0.5 : 0.807692307692\n",
    "AP score for threshold 0.55 : 0.807692307692\n",
    "AP score for threshold 0.6 : 0.740740740741\n",
    "AP score for threshold 0.65 : 0.740740740741\n",
    "AP score for threshold 0.7 : 0.740740740741\n",
    "AP score for threshold 0.75 : 0.620689655172\n",
    "AP score for threshold 0.8 : 0.424242424242\n",
    "AP score for threshold 0.85 : 0.146341463415\n",
    "AP score for threshold 0.9 : 0.0444444444444\n",
    "Mean AP score is:  0.56370275832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP over multiple runs for MetaSeg is: [0.95450889982757214, 0.95077826549312294, 0.94659663872276345, 0.9388713306622426, 0.9130383431717114, 0.83221739785150184, 0.65271381498766978, 0.35376628510352454, 0.096076682513008188]\n"
     ]
    }
   ],
   "source": [
    "average_precision_score_per_run = []\n",
    "thresholds = np.arange(0.5,0.95,0.05)\n",
    "runs = [1,2,3,4]\n",
    "for run_idx in runs:\n",
    "    average_precision_score_list = []\n",
    "    for thresh in thresholds:\n",
    "\n",
    "        gt_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/SEG/*.tif\")\n",
    "        label_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/BIC-MetaSeg/run\"+str(run_idx)+\"/67annotations/*.tif\")\n",
    "        sum_seg = 0\n",
    "        scores_per_time = []\n",
    "        for i in range(len(gt_segs_path)):\n",
    "            score = precision(imread(label_segs_path[i]).astype(np.uint16), imread(gt_segs_path[i]), thresh)\n",
    "            scores_per_time.append(score)\n",
    "            if(0<=score<=1):\n",
    "                sum_seg = sum_seg + score\n",
    "\n",
    "        average_precision_score = sum_seg/len(gt_segs_path)\n",
    "        average_precision_score_list.append(average_precision_score)\n",
    "#         print(\"AP score for threshold \"+str(thresh)+\" :\", average_precision_score )\n",
    "    average_precision_score_per_run.append(average_precision_score_list)\n",
    "# print(\"Mean AP score is: \", np.mean(np.array(average_precision_score_list)))\n",
    "\n",
    "\n",
    "\n",
    "mean_average_precision_score_per_threshold = []\n",
    "for j in range(len(thresholds)):\n",
    "    s = 0\n",
    "    for i in range(len(average_precision_score_per_run)):\n",
    "        s=s+average_precision_score_per_run[i][j]   \n",
    "    mean_average_precision_score_per_threshold.append(s/len(runs))\n",
    "print(\"AP over multiple runs for MetaSeg is:\", mean_average_precision_score_per_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEG score : 0.834807124178\n",
      "DICE score : 0.909687929998\n",
      "SEG score : 0.829868240698\n",
      "DICE score : 0.906709900285\n",
      "SEG score : 0.824392690661\n",
      "DICE score : 0.90330722363\n",
      "SEG score : 0.830044766094\n",
      "DICE score : 0.906806216752\n",
      "SEG score over multiple runs for MetaSeg: 0.829778205408\n",
      "DICE score over multiple runs for MetaSeg: 0.906627817666\n"
     ]
    }
   ],
   "source": [
    "runs = [1,2,3,4]\n",
    "seg_score_per_run = []\n",
    "dice_score_per_run = []\n",
    "for run_idx in runs:\n",
    "\n",
    "    gt_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/SEG/*.tif\")\n",
    "    label_segs_path = glob(\"/Users/prakash/Desktop/MetaSeg_Data_Version1/CTC_GOWT1/BIC-MetaSeg/run\"+str(run_idx)+\"/67annotations/*.tif\")\n",
    "\n",
    "    seg_scores_per_time = []\n",
    "    dice_scores_per_time = []\n",
    "    for i in range(len(gt_segs_path)):\n",
    "        seg_score, dice_score = seg(imread(gt_segs_path[i]), imread(label_segs_path[i]).astype(np.uint16))\n",
    "    #     print(seg_score, dice_score)\n",
    "        seg_scores_per_time.append(seg_score)\n",
    "        dice_scores_per_time.append(dice_score)\n",
    "\n",
    "    print(\"SEG score :\", np.mean(np.array(seg_scores_per_time) ))\n",
    "    print(\"DICE score :\", np.mean(np.array(dice_scores_per_time) ) )\n",
    "    seg_score_per_run.append(np.mean(np.array(seg_scores_per_time) ))\n",
    "    dice_score_per_run.append(np.mean(np.array(dice_scores_per_time) ))\n",
    "\n",
    "\n",
    "print(\"SEG score over multiple runs for MetaSeg:\", np.mean(np.array(seg_score_per_run)))\n",
    "print(\"DICE score over multiple runs for MetaSeg:\", np.mean(np.array(dice_score_per_run)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
